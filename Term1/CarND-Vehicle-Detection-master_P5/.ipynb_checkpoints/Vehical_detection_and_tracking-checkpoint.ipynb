{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection and Tracking\n",
    "## Udacity Self Driving Car Engineer Nanodegree - Project 5\n",
    "\n",
    "In the previous file we have trained the data and save, now we will import the saved data and \n",
    "- Identify vehicles in images\n",
    "- Track vehicles across frames in a video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from moviepy.editor import VideoFileClip\n",
    "%matplotlib inline\n",
    "\n",
    "#importin from the previous file\n",
    "from Preprocessing_training_dataset import draw_boxes\n",
    "from Preprocessing_training_dataset import features_extration_single_image\n",
    "from Preprocessing_training_dataset import sliding_window\n",
    "\n",
    "%matplotlib inline\n",
    "print('Import Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MLP and Scaler\n",
    "mlp = joblib.load('MLP_classifier.pkl')\n",
    "X_scaler = joblib.load('scaler_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.75, 0.75)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_windows = np.int(xspan/nx_pix_per_step) \n",
    "    ny_windows = np.int(yspan/ny_pix_per_step)\n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = (xs+1)*nx_pix_per_step + x_start_stop[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = (ys+1)*ny_pix_per_step + y_start_stop[0]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotate(image):\n",
    "    image = imread(image)\n",
    "    detected = []\n",
    "    size = 320\n",
    "    count = 0\n",
    "    while size < 720:\n",
    "        windows = sliding_window(image, x_start_stop=[None, None], y_start_stop=[400, 660], \n",
    "                            xy_window=(size, size), xy_overlap=(0.8, 0.8))  \n",
    "        for window in windows:\n",
    "            features = []\n",
    "            current = cv2.resize((image[window[0][1]: window[1][1], window[0][0]: window[1][0]]),(64,64))\n",
    "            hog_features = features_extration_single_image(current,color_space='YUV')\n",
    "            scaled_features = X_scaler.transform(hog_features)\n",
    "            if current.shape[0] > 0:\n",
    "                if mlp.predict_proba(scaled_features.reshape(1,-1))[0][1] > .99:\n",
    "                    detected.append(window)\n",
    "            count += 1\n",
    "        size += 16\n",
    "    result = np.copy(image)\n",
    "    mask = np.zeros_like(image)\n",
    "    # Draw all of the boxes on a mask image\n",
    "    mask = draw_boxes(mask, bboxes=detected, thick=-1)\n",
    "    # Find the contours in the mask\n",
    "    im2, contours, hierarchy = cv2.findContours(mask[:,:,2].astype('uint8'),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        # Get the coordinates of a bounding rect for each contour\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        # Draw the bounding rectangles on the result image\n",
    "        cv2.rectangle(result, (x, y), (x + w, y + h), (255, 0, 255), 6)\n",
    "       # M = cv2.moments(cnt)\n",
    "       # c = (int(M['m10'] / M['m00']), int(M['m01'] / M['m00']))\n",
    "       # cv2.circle(result, c, 15, (255, 0, 0), -1)\n",
    "    f, (ax1,ax2, ax3) = plt.subplots(1,3, figsize=(10,6))\n",
    "    f.tight_layout()\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Image')\n",
    "    ax1.imshow(image)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Car Detections')\n",
    "    ax2.imshow(mask, cmap='hot')\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('Annotated Image')\n",
    "    ax3.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for image in glob.glob('test_images/*.jpg'):\n",
    "    annotate(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class boxes:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.detections = deque(maxlen=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_vid(image):\n",
    "    detected = [] \n",
    "    size = 320\n",
    "    count = 0\n",
    "    while size < 720:\n",
    "        windows = sliding_window(image, x_start_stop=[640, None], y_start_stop=[400, 660], \n",
    "                            xy_window=(size, size), xy_overlap=(0.8, 0.8))  \n",
    "        for window in windows:\n",
    "            features = []\n",
    "            current = cv2.resize((image[window[0][1]: window[1][1], window[0][0]: window[1][0]]),(64,64))\n",
    "            hog_features = features_extration_single_image(current, color_space='YUV')\n",
    "            scaled_features = X_scaler.transform(hog_features)\n",
    "            if current.shape[0] > 0:\n",
    "                if mlp.predict_proba(scaled_features.reshape(1,-1))[0][1] > .99:\n",
    "                    detected.append(window)\n",
    "            count += 1\n",
    "        size += 16\n",
    "    result = np.copy(image).astype('uint8')\n",
    "    mask = np.zeros_like(image)\n",
    "    mask = draw_boxes(mask, bboxes=detected, thick=-1)\n",
    "    rect_list = []\n",
    "    im2, contours, hierarchy = cv2.findContours(mask[:,:,2].astype('uint8'),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        rect_list.append([x,y,x+w,y+h])\n",
    "    Boxes.detections.append(rect_list)\n",
    "    all_boxes = []\n",
    "    combined = np.ravel(np.array(Boxes.detections))\n",
    "    for i in range(len(combined)):\n",
    "        all_boxes.extend(np.ravel(combined[i]))\n",
    "    new_boxes = []\n",
    "    i = 0\n",
    "    while i <= len(all_boxes)-3:\n",
    "        new_boxes.append(all_boxes[i:i+4])\n",
    "        i += 4\n",
    "    rects,w = cv2.groupRectangles(np.array(new_boxes).tolist(), 10,.1)\n",
    "    for rect in rects:\n",
    "        cv2.rectangle(result, (rect[0], rect[1]), (rect[2],rect[3]), (255,0,255), 5)\n",
    "    Boxes.count += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Boxes = boxes()\n",
    "output = 'result.mp4'\n",
    "clip1 = VideoFileClip('project_video.mp4').subclip(5,) # The first 5 seconds doesn't have any cars...\n",
    "clip = clip1.fl_image(process_vid)\n",
    "%time clip.write_videofile(output, audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
